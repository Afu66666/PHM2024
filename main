import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader as TorchDataLoader
from sklearn.model_selection import train_test_split
import os
from pathlib import Path
from data_loader import DataLoader
from data_processor import (DataProcessor, MultiComponentDataset)
from load_simulation import augment_with_load_simulation
from spec_augment import augment_dataset_with_strategies
from model_pretrain import (
    HuggingFaceModelManager, 
    create_branch_with_hf_weights, 
    ImprovedMultiComponentModel,
    train_model,
    evaluate_model
)
from cache_utils import cache_exists, load_from_cache, save_to_cache, save_stft_data, load_stft_data
import timm
import torch.nn.functional as F

# 设置随机种子，确保结果可重现
def set_seed(seed=42):
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# 调整STFT图像大小的函数
def resize_stft_images(stft_images, target_size=(224, 224)):
    resized_list = []
    for component_data in stft_images:
        tensor_data = torch.from_numpy(component_data).float()
        resized = F.interpolate(
            tensor_data, 
            size=target_size,
            mode='bilinear',
            align_corners=False
        )
        resized_list.append(resized.numpy())
    return resized_list

def main():
    # 设置随机种子
    set_seed(42)
    
    # 检查CUDA是否可用
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(f"使用设备: {device}")
    
    # 指定timm缓存目录
    timm_cache_dir = os.path.expanduser("~/timm_cache")
    os.makedirs(timm_cache_dir, exist_ok=True)
    os.environ['TORCH_HOME'] = timm_cache_dir
    print(f"timm模型将缓存到: {timm_cache_dir}")
    
    # 创建模型管理器（为了兼容性）
    hf_cache_dir = os.path.expanduser("~/huggingface_models")
    hf_manager = HuggingFaceModelManager(cache_dir=hf_cache_dir)

    ############################################################################
    ######################### 数据加载逻辑 #####################################
    ############################################################################
    # 初始化处理器和必要变量
    processor = DataProcessor(scaler_type='robust')
    components = None
    num_classes = None
    
    # 1. 检查是否存在增强数据缓存
    print("\n开始检查各级缓存数据...")
    augmented_cache_exists = cache_exists("augmented_data")
    augmented_cache_exists = False
    
    if augmented_cache_exists:
        print("【1/4】发现增强数据缓存，直接加载...")
        
        # 加载增强数据缓存
        augmented_cache = load_from_cache("augmented_data")
        
        # 提取所有必要数据
        train_components = augmented_cache['train_components']
        val_components = augmented_cache['val_components']
        test_components = augmented_cache['test_components']
        train_labels = augmented_cache['train_labels']
        val_labels = augmented_cache['val_labels']
        test_labels = augmented_cache['test_labels']
        train_conditions = augmented_cache['train_conditions']
        val_conditions = augmented_cache['val_conditions']
        test_conditions = augmented_cache['test_conditions']
        components = augmented_cache['components']
        num_classes = augmented_cache.get('num_classes', len(np.unique(train_labels)))
        
        print(f"增强数据加载完成: {len(train_labels)}训练样本, {len(val_labels)}验证样本, {len(test_labels)}测试样本")
        print(f"数据形状: {train_components[0].shape}")
        
        # 跳过所有其他数据处理步骤
        skip_all_processing = True
        
    else:
        # 2. 检查是否存在STFT数据缓存
        stft_cache_exists = cache_exists("stft_data")
        stft_cache_exists = False
        
        if stft_cache_exists:
            print("【2/4】未找到增强数据缓存，但发现STFT数据缓存，将加载STFT数据...")
            
            # 加载STFT数据缓存
            stft_data = load_stft_data()
            
            # 提取所有必要数据
            train_components = stft_data['train_components']
            val_components = stft_data['val_components']
            test_components = stft_data['test_components']
            train_conditions = stft_data['train_conditions']
            val_conditions = stft_data['val_conditions']
            test_conditions = stft_data['test_conditions']
            train_labels = stft_data['train_labels']
            val_labels = stft_data['val_labels']
            test_labels = stft_data['test_labels']
            components = stft_data['components']
            
            # 尝试获取num_classes
            train_cache_exists = cache_exists("training_data")
            if train_cache_exists:
                train_data = load_from_cache("training_data")
                fault_types = train_data['fault_types']
                num_classes = len(fault_types)
            else:
                num_classes = len(np.unique(train_labels))
            
            print(f"STFT数据加载完成: {len(train_labels)}训练样本, {len(val_labels)}验证样本, {len(test_labels)}测试样本")
            
            # 需要调整大小和进行数据增强
            skip_raw_loading = True
            skip_split_before_stft = True  # 已经是STFT后的数据
            skip_stft_processing = True
            
        else:
            # 3. 检查是否存在原始数据缓存
            train_cache_exists = cache_exists("training_data")
            test_cache_exists = cache_exists("testing_data")
            
            if train_cache_exists and test_cache_exists:
                print("【3/4】未找到STFT数据缓存，但发现原始数据缓存，将加载原始数据...")
                
                # 加载训练数据缓存
                train_data = load_from_cache("training_data")
                component_arrays_train = train_data['component_arrays']
                train_labels_full = train_data['labels'] # 重命名保持一致
                conditions_train_full = train_data['conditions']
                fault_types = train_data['fault_types']
                num_classes = len(fault_types)
                
                # 加载测试数据缓存
                test_data = load_from_cache("testing_data")
                component_arrays_test = test_data['component_arrays']
                test_labels = test_data['labels'] # 重命名保持一致
                conditions_test = test_data['conditions']
                
                # 获取组件列表
                components = list(component_arrays_train.keys())
                
                print(f"原始数据加载完成: {len(train_labels_full)}训练样本, {len(test_labels)}测试样本")
                
                # 需要划分训练集和验证集，再进行工况增强、STFT等处理
                skip_raw_loading = True
                skip_split_before_stft = False
                skip_stft_processing = False
                
            else:
                # 4. 直接从文件读取原始数据
                print("【4/4】未找到任何缓存，将从文件读取原始数据...")
                
                # 初始化数据加载器
                loader = DataLoader("Datasets")
                
                # 获取故障类型
                fault_types = loader.get_fault_types()
                print(f"发现故障类型: {fault_types}")
                num_classes = len(fault_types)
                
                # 加载训练数据
                print("正在加载训练数据...")
                component_arrays_train, train_labels_full, conditions_train_full = loader.get_processed_dataset(dataset="Training")
                
                # 加载测试数据
                print("正在加载测试数据...")
                component_arrays_test, test_labels, conditions_test = loader.get_processed_dataset(dataset="Test")
                
                # 获取组件列表
                components = list(component_arrays_train.keys())
                
                # 保存原始数据缓存
                train_cache = {
                    'component_arrays': component_arrays_train,
                    'labels': train_labels_full,
                    'conditions': conditions_train_full,
                    'fault_types': fault_types
                }
                save_to_cache(train_cache, "training_data")
                
                test_cache = {
                    'component_arrays': component_arrays_test,
                    'labels': test_labels,
                    'conditions': conditions_test
                }
                save_to_cache(test_cache, "testing_data")
                
                print(f"原始数据加载完成并已缓存: {len(train_labels_full)}训练样本, {len(test_labels)}测试样本")
                
                # 需要划分训练集和验证集，再进行工况增强、STFT等处理
                skip_raw_loading = True
                skip_split_before_stft = False
                skip_stft_processing = False
                
        skip_all_processing = False

    ############################################################################
    ######################### 训练集和验证集划分 ###############################
    ############################################################################
    if not skip_all_processing:
        if not skip_split_before_stft:
            print("\n在任何增强之前先划分训练集和验证集...")
            
            # 从训练集中划分出验证集
            train_indices, val_indices = train_test_split(
                np.arange(len(train_labels_full)), 
                test_size=0.4,  # 训练集的40%作为验证集
                random_state=42,
                stratify=train_labels_full  # 保证各类别比例一致
            )
            
            print(f"原始数据划分: 训练集={len(train_indices)}样本, 验证集={len(val_indices)}样本, 测试集={len(test_labels)}样本")
            
            # 分割标签
            train_labels = train_labels_full[train_indices]
            val_labels = train_labels_full[val_indices]
            
            # 分割工况数据
            train_conditions_raw = conditions_train_full[train_indices]
            val_conditions_raw = conditions_train_full[val_indices]
            
            # 分割组件数据
            train_components_dict = {}
            val_components_dict = {}
            
            for component in components:
                train_components_dict[component] = component_arrays_train[component][train_indices]
                val_components_dict[component] = component_arrays_train[component][val_indices]
            
            print(f"训练集标签分布: {np.unique(train_labels, return_counts=True)}")
            print(f"验证集标签分布: {np.unique(val_labels, return_counts=True)}")
                
        ############################################################################
        ######################### 工况平衡增强 #####################################
        ############################################################################
        if not skip_stft_processing:
            print("\n仅对训练集进行工况平衡增强,模拟未知工况...")
            
            # 确保工况条件数组格式正确
            print(f"原始训练集工况条件数组形状: {train_conditions_raw.shape}")
            print(f"原始训练集工况负载分布: {np.unique(train_conditions_raw[:, 1], return_counts=True)}")
            print(f"原始训练集样本数: {len(train_labels)}")
            
            # 仅对训练集应用负载模拟增强
            augmented_components, augmented_conditions, augmented_labels = augment_with_load_simulation(
                train_components_dict,  # 原始时域数据字典
                train_conditions_raw,   # 原始工况
                train_labels,           # 原始标签
                num_augmented=2         # 每个样本生成2个新样本( 1个0kN,1个1kN)
            )
            
            # 更新训练集数据
            train_components_dict = augmented_components
            train_conditions_raw = augmented_conditions
            train_labels = augmented_labels
            
            print(f"工况增强后训练集样本数: {len(train_labels)}")
            print(f"工况增强后训练集负载分布: {np.unique(train_conditions_raw[:, 1], return_counts=True)}")
            
            ############################################################################
            ######################### STFT转换 #########################################
            ############################################################################
            print("\n对各部件数据进行STFT变换...")

            # 设置STFT参数
            stft_params = {
                'fs': 1000,         # 采样率Hz
                'nperseg': 1024,    # 窗口长度
                'noverlap': 768,    # 重叠点数 (75%重叠)
                'window': 'hann',   # 窗函数
                'detrend': False,   # 不去趋势
            }

            # 检测训练集和测试集的时间长度差异
            train_length = next(iter(train_components_dict.values())).shape[1]
            val_length = next(iter(val_components_dict.values())).shape[1]
            test_length = next(iter(component_arrays_test.values())).shape[1]
            print(f"检测到样本长度: 训练集={train_length}, 验证集={val_length}, 测试集={test_length}")

            # 初始化组件列表和STFT结果列表
            train_components = []
            val_components = []
            test_components = []

            # 提前编码所有工况，避免重复编码
            print("编码工况数据")
            train_conditions = processor.encode_conditions_batch(train_conditions_raw)
            val_conditions = processor.encode_conditions_batch(val_conditions_raw)
            test_conditions = processor.encode_conditions_batch(conditions_test)
            print("工况编码完成")

            # 使用批处理来提高效率
            batch_size = 16  # 可调整的批大小

            # 处理训练集各部件数据
            for component_idx, component in enumerate(components):
                print(f"处理 {component} 训练数据... ({component_idx+1}/{len(components)})")
                
                # 对该部件的所有样本应用STFT
                component_data = train_components_dict[component]
                total_samples = component_data.shape[0]
                all_stft = []
                
                # 使用批处理处理样本
                for batch_start in range(0, total_samples, batch_size):
                    batch_end = min(batch_start + batch_size, total_samples)
                    batch_size_actual = batch_end - batch_start
                    
                    # 打印处理进度
                    if batch_start % (batch_size * 10) == 0 or batch_start + batch_size >= total_samples:
                        print(f"  - 处理样本 {batch_start}-{batch_end-1}/{total_samples}")
                    
                    # 准备批数据
                    batch_samples = component_data[batch_start:batch_end]
                    batch_conditions = train_conditions_raw[batch_start:batch_end]
                    batch_labels = train_labels[batch_start:batch_end] if train_labels is not None else None
                    
                    # 批量应用STFT
                    batch_stft_result, _, _ = processor.process_dataset(
                        batch_samples, 
                        batch_conditions, 
                        batch_labels,
                        component_name=component,
                        apply_stft=True, 
                        stft_params=stft_params
                    )
                    
                    # 将批次结果添加到列表
                    for i in range(batch_size_actual):
                        all_stft.append(batch_stft_result[i])
                
                # 堆叠所有样本的STFT结果
                train_components.append(np.stack(all_stft))
                print(f"  - 完成 {component} 训练数据，形状: {train_components[-1].shape}")
                
            # 处理验证集各部件数据
            for component_idx, component in enumerate(components):
                print(f"处理 {component} 验证数据... ({component_idx+1}/{len(components)})")
                
                # 对该部件的所有样本应用STFT
                component_data = val_components_dict[component]
                total_samples = component_data.shape[0]
                all_stft = []
                
                # 使用批处理处理样本
                for batch_start in range(0, total_samples, batch_size):
                    batch_end = min(batch_start + batch_size, total_samples)
                    batch_size_actual = batch_end - batch_start
                    
                    # 打印处理进度
                    if batch_start % (batch_size * 10) == 0 or batch_start + batch_size >= total_samples:
                        print(f"  - 处理样本 {batch_start}-{batch_end-1}/{total_samples}")
                    
                    # 准备批数据
                    batch_samples = component_data[batch_start:batch_end]
                    batch_conditions = val_conditions_raw[batch_start:batch_end]
                    batch_labels = val_labels[batch_start:batch_end] if val_labels is not None else None
                    
                    # 批量应用STFT
                    batch_stft_result, _, _ = processor.process_dataset(
                        batch_samples, 
                        batch_conditions, 
                        batch_labels,
                        component_name=component,
                        apply_stft=True, 
                        stft_params=stft_params
                    )
                    
                    # 将批次结果添加到列表
                    for i in range(batch_size_actual):
                        all_stft.append(batch_stft_result[i])
                
                # 堆叠所有样本的STFT结果
                val_components.append(np.stack(all_stft))
                print(f"  - 完成 {component} 验证数据，形状: {val_components[-1].shape}")

            # 处理测试集各部件数据
            for component_idx, component in enumerate(components):
                print(f"处理 {component} 测试数据... ({component_idx+1}/{len(components)})")
                
                # 对该部件的所有样本应用STFT
                component_data = component_arrays_test[component]
                total_samples = component_data.shape[0]
                all_stft = []
                
                # 使用批处理处理样本
                for batch_start in range(0, total_samples, batch_size):
                    batch_end = min(batch_start + batch_size, total_samples)
                    batch_size_actual = batch_end - batch_start
                    
                    # 打印处理进度
                    if batch_start % (batch_size * 10) == 0 or batch_start + batch_size >= total_samples:
                        print(f"  - 处理样本 {batch_start}-{batch_end-1}/{total_samples}")
                    
                    # 准备批数据
                    batch_samples = component_data[batch_start:batch_end]
                    batch_conditions = conditions_test[batch_start:batch_end]
                    batch_labels = test_labels[batch_start:batch_end] if test_labels is not None else None
                    
                    # 批量应用STFT
                    batch_stft_result, _, _ = processor.process_dataset(
                        batch_samples, 
                        batch_conditions, 
                        batch_labels,
                        component_name=component,
                        apply_stft=True, 
                        stft_params=stft_params
                    )
                    
                    # 将批次结果添加到列表
                    for i in range(batch_size_actual):
                        all_stft.append(batch_stft_result[i])
                
                # 堆叠所有样本的STFT结果
                test_components.append(np.stack(all_stft))
                print(f"  - 完成 {component} 测试数据，形状: {test_components[-1].shape}")

            # STFT处理完毕后保存数据
            print("\n保存训练集、验证集和测试集的STFT数据...")
            save_stft_data(
                train_components=train_components,
                val_components=val_components,
                test_components=test_components,
                train_conditions=train_conditions,
                val_conditions=val_conditions,
                test_conditions=test_conditions,
                train_labels=train_labels,
                val_labels=val_labels,
                test_labels=test_labels,
                components=components,
                stft_params=stft_params
            )

            # 打印STFT处理后的形状
            print("\nSTFT处理完成，各部件STFT形状:")
            for i, comp_data in enumerate(train_components):
                print(f"训练集 - 部件 {components[i]}: {comp_data.shape}")
            for i, comp_data in enumerate(val_components):
                print(f"验证集 - 部件 {components[i]}: {comp_data.shape}")
            for i, comp_data in enumerate(test_components):
                print(f"测试集 - 部件 {components[i]}: {test_components[-1].shape}")

        ############################################################################
        ######################### 数据增强 #########################################
        ############################################################################
        print("\n仅对训练集使用SpecAugment进行数据增强...")
        
        # 确保标签和数据长度一致
        data_length = train_components[0].shape[0]
        if len(train_labels) != data_length:
            print(f"警告: 标签长度 ({len(train_labels)}) 与数据长度 ({data_length}) 不匹配，将截断标签")
            train_labels = train_labels[:data_length]
            # 同时调整工况数据
            if isinstance(train_conditions, dict) and len(train_conditions["freq_id"]) != data_length:
                train_conditions = {
                    "freq_id": train_conditions["freq_id"][:data_length],
                    "load_id": train_conditions["load_id"][:data_length]
                }
        
        # 记录原始数据量
        original_len = len(train_labels)
        
        # 只对训练集应用SpecAugment增强
        train_components, train_labels = augment_dataset_with_strategies(
            train_components,   # 原始组件数据
            train_labels,               # 原始标签
            components,                 # 组件名称列表
            samples_per_class=5         # 每类生成5个额外样本
        )
        
        # 计算新增了多少样本
        new_len = len(train_labels)
        added_samples = new_len - original_len
        print(f"训练集从 {original_len} 增加到 {new_len} 样本 (增加了 {added_samples} 样本)")
        
        # 同步更新工况数据 - 为新样本复制工况信息
        # 找出每个类别对应的索引
        unique_classes = np.unique(train_labels[:original_len])
        
        # 为每个生成的样本选择对应类别的一个随机工况
        freq_id_extended = list(train_conditions["freq_id"])
        load_id_extended = list(train_conditions["load_id"])
        
        # 为每个新样本分配工况
        for i in range(original_len, new_len):
            class_idx = train_labels[i]
            # 找出该类别的所有原始样本索引
            class_indices = np.where(train_labels[:original_len] == class_idx)[0]
            # 随机选择一个索引作为工况参考
            if len(class_indices) > 0:
                ref_idx = np.random.choice(class_indices)
                # 复制该索引的工况
                freq_id_extended.append(train_conditions["freq_id"][ref_idx])
                load_id_extended.append(train_conditions["load_id"][ref_idx])
        
        # 更新工况数据为扩展后的结构
        train_conditions = {
            "freq_id": np.array(freq_id_extended),
            "load_id": np.array(load_id_extended),
        }
        
        train_components = train_components
        train_labels = train_labels
        val_components = val_components
        val_labels = val_labels
        
        print("工况数据已同步更新，与增强后样本数匹配")
        
        # ############################################################################
        # ######################### 缓存增强后的数据 ################################
        # ############################################################################
        print("\n准备缓存增强后的数据...")
        
        # 准备缓存数据 - 注意这里保存的是已经划分好并且增强过的数据
        augmented_cache = {
            'train_components': train_components,
            'val_components': val_components,
            'test_components': test_components,
            'train_labels': train_labels,
            'val_labels': val_labels,
            'y_test': test_labels,
            'train_conditions': train_conditions,
            'val_conditions': val_conditions,
            'test_conditions': test_conditions,
            'components': components,
            'num_classes': num_classes
        }
        
        # 保存缓存
        save_to_cache(augmented_cache, "augmented_data")
        print("增强后的数据已成功缓存，下次运行时可直接加载使用。")

    ############################################################################
    ######################### 创建多部件PyTorch数据集 ##########################
    ############################################################################
    # 创建MultiComponentDataset实例
    train_dataset = MultiComponentDataset(
        train_components, train_conditions, train_labels, 
        transform=None, is_stft=True
    )
    
    val_dataset = MultiComponentDataset(
        val_components, val_conditions, val_labels,
        transform=None, is_stft=True
    )
    
    test_dataset = MultiComponentDataset(
        test_components, test_conditions, test_labels,
        transform=None, is_stft=True
    )
    
    # 创建DataLoader
    batch_size = 16
    
    train_loader = TorchDataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True,
        num_workers=4,
        pin_memory=torch.cuda.is_available()
    )
    
    val_loader = TorchDataLoader(
        val_dataset, 
        batch_size=batch_size, 
        shuffle=False,
        num_workers=4,
        pin_memory=torch.cuda.is_available()
    )
    
    test_loader = TorchDataLoader(
        test_dataset, 
        batch_size=batch_size, 
        shuffle=False,
        num_workers=4,
        pin_memory=torch.cuda.is_available()
    )
    
    print(f"\n数据加载器创建完成: 每批{batch_size}个样本")
    
    # 查看一个批次的数据样式
    for inputs, freq_ids, load_ids, labels in train_loader:
        print("\n示例批次数据:")
        for i, component_tensor in enumerate(inputs):
            print(f"部件{i}张量形状: {component_tensor.shape}")
        print(f"频率ID形状: {freq_ids.shape}")
        print(f"负载ID形状: {load_ids.shape}")
        print(f"标签形状: {labels.shape}")
        break
    
    ############################################################################
    ######################### 创建和训练多部件模型 #############################
    ############################################################################
    # 获取各部件通道数
    component_channels = [data.shape[1] for data in train_components]
    
    # 使用预训练模型的选择
    model_type = 'resnet18'
    
    # 创建组件分支
    print(f"\n使用timm创建{model_type}模型分支...")
    component_branches = []
    for i, num_channels in enumerate(component_channels):
        print(f"为部件{i} (通道数={num_channels})创建分支...")
        branch = create_branch_with_hf_weights(
            model_type, 
            num_channels, 
            pretrained=True,
            hf_manager=hf_manager
        )
        component_branches.append(branch)
    
    # 创建增强模型
    model = ImprovedMultiComponentModel(
        branches=component_branches,
        num_classes=num_classes,
        working_condition=True
    )
    print(f"\n创建了增强的基于{model_type}的多部件模型，包含工况适应功能")
    
    model = model.to(device)
    
    # 输出模型摘要
    print(f"\n模型架构:")
    print(f"- {len(model.component_branches)}个部件分支网络")
    print(f"- 每个部件的通道数: {component_channels}")
    print(f"- 包含工况信息: 是")
    print(f"- 使用增强工况适应器: 是")
    
    # 训练模型
    print("开始训练模型...")
    trained_model, history = train_model(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        device=device,
        epochs=50,
        lr=0.001,
        weight_decay=0.01,
        freeze_pretrained=True,
        unfreeze_layers=[
             "layer3","layer4", "condition_adapters", "condition_fusion", 
            "visual_fusion", "final_fusion"
        ]
    )
    # 绘制训练历史
    from plot_training_history import plot_training_history
    plot_training_history(
        history=history,
        save_path="training_curves.png",
        show_plot=True
    )
    
    # 在测试集上评估模型
    print("\n在测试集上评估模型...")
    test_acc, predictions, targets = evaluate_model(
        model=trained_model,
        test_loader=test_loader,
        device=device
    )

    # 按工况分析性能
    print("\n按工况分解测试性能:")
    test_freqs = []
    test_loads = []
    test_predictions = []
    test_ground_truth = []

    # 收集测试集预测和工况信息
    with torch.no_grad():
        for inputs, freq_ids, load_ids, targets in test_loader:
            # 将数据移至设备
            inputs = [x.to(device) for x in inputs]
            freq_ids = freq_ids.to(device)
            load_ids = load_ids.to(device)
            targets = targets.to(device).long().squeeze()
            
            # 前向传播
            outputs = trained_model(inputs, freq_ids, load_ids)
            
            # 获取预测
            _, predicted = outputs.max(1)
            
            # 收集数据
            test_freqs.extend(freq_ids.cpu().numpy().flatten())
            test_loads.extend(load_ids.cpu().numpy().flatten())
            test_predictions.extend(predicted.cpu().numpy())
            test_ground_truth.extend(targets.cpu().numpy())

    # 频率和负载ID到实际值的映射
    freq_map = {0: "20Hz", 1: "40Hz", 2: "60Hz"}
    load_map = {0: "0kN", 1: "1kN", 2: "-1kN"}

    # 将列表转换为NumPy数组
    test_freqs = np.array(test_freqs)
    test_loads = np.array(test_loads)
    test_predictions = np.array(test_predictions)
    test_ground_truth = np.array(test_ground_truth)

    # 分析每种工况下的性能
    unique_freqs = np.unique(test_freqs)
    unique_loads = np.unique(test_loads)

    print("\n各工况测试集性能:")
    for freq in unique_freqs:
        for load in unique_loads:
            # 找出该工况下的所有样本
            indices = np.where((test_freqs == freq) & (test_loads == load))[0]
            if len(indices) > 0:
                # 计算该工况下的准确率
                correct = np.sum(test_predictions[indices] == test_ground_truth[indices])
                accuracy = 100.0 * correct / len(indices)
                print(f"工况 {freq_map[freq]}, {load_map[load]}: 准确率 = {accuracy:.2f}% ({correct}/{len(indices)})")

    # 保存模型
    save_path = f"multi_component_{model_type}_model.pth"
    torch.save(trained_model.state_dict(), save_path)
    print(f"\n模型已保存至 {save_path}")
    
    print("\n训练和评估完成!")

if __name__ == "__main__":
    main()
